---
title: "Chapter11_textbook"
author: "Lin Zhang"
date: "10/30/2019"
output: html_document
---

```{r setup, include=FALSE}
library(rethinking)
```

```{r load data}
data("chimpanzees")
d <- chimpanzees
```

```{r data examine}
unique(d$actor)
unique(d$recipient)
unique(d$block)
unique(d$prosoc_left)
# trial: ordinal sequence of trials for each chimp
# ? choice chimp made (0=1/0 option, 1=1/1 option) ??
```

```{r predic pulled_left as outcome with prosoc_left and condition as predictor}
d$treatment <- 1+ d$prosoc_left+2*d$condition
xtabs(~treatment+prosoc_left+condition,d)
# display of analysis design?
```
# model building exploration
# prior predictive simulation
```{r build a example Binomial model}
m11.1 <- quap(
  alist(
    pulled_left ~ dbinom(1,p),
    logit(p) <- a,
    a ~ dnorm(0,10)
  ),data = d
)
```

```{r sample from the prior}
set.seed(1999)
prior <- extract.prior(m11.1,n = 1e4)
```

```{r inverse-link}
# convert the parameter to the outcome scale
p <- inv_logit(prior$a)
dens(p,adj = 0.1) # should change a from norm(0,10) to norm(0,1.5)
```

```{r model with prior}
m11.2 <- quap(
  alist(
    pulled_left ~ dbinom(1,p),
    logit(p) <- a+b[treatment],
    a ~ dnorm(0,1.5),
    b[treatment] ~ dnorm(0,10)
  ),data = d
)
```

```{r extract prior}
set.seed(1999)
prior <- extract.prior(m11.2,n=1e4)
p <- sapply(1:4,function(k)inv_logit(prior$a+prior$b[,k]))
# plot the absolute prior difference between first two treatments
dens(abs(p[,1]-p[,2]),adj = 0.1)
```

```{r regularize the prior}
m11.3 <- quap(alist(
pulled_left ~ dbinom( 1 , p ) ,
logit(p) <- a + b[treatment] ,
a ~ dnorm( 0 , 1.5 ),
b[treatment] ~ dnorm( 0 , 0.5 )
) , data=d )
set.seed(1999)
prior <- extract.prior( m11.3 , n=1e4 )
p <- sapply( 1:4 , function(k) inv_logit( prior$a + prior$b[,k] ) )
dens(abs(p[,1]-p[,2]),adj = 0.1) # what the priors imply about the prior differenes among treatments, plot absolute prior difference between the first two treatments
mean(abs(p[,1]-p[,2])) # why this is not a strong prior, if the data contain evidence of large differences, they will shine through? Good priors hurt fit to sample but are expected to improve prediction
```
# build the actual model 
```{r}
# add in all the individual chimpanzee parameters
# HMC to approximate the posterior
# prior trimmed data list
dat_list <- list(
  pulled_left =d$pulled_left,
  actor=d$actor,
  treatment=as.integer(d$treatment)
)
# particles in 11-dimensional space #7 chimap individuals, 4 treatment, 7+4-> dimension ? why
m11.4 <- ulam(
  alist(
    pulled_left ~ dbinom(1,p),
    logit(p) <- a[actor]+b[treatment], # if indicator variable has a disadvantage of adding uncertainty to one category than the other, why are we turning to indicator all the time?
    a[actor] ~ dnorm(0,1.5),
    b[treatment] ~ dnorm(0,0.5)
  ),
  data = dat_list,chains = 4, log_lik = TRUE
)
precis(m11.4, depth = 2)
plot(precis(m11.4, depth = 2))
```

```{r examine individual chimp effect}
post <- extract.samples(m11.4)
p_left <- inv_logit(post$a) # not a dataframe?
plot(precis(as.data.frame(p_left)),xlim=c(0,1))
```

```{r examine treatment effects}
labs <- c("R/N","L/N","R/P","L/P")
plot(precis(m11.4,depth = 2,pars = "b"),labels=labs)
# is the individual effect of chimp conditoned on alreadly in this case?
# in above figure, 4 chimps tend to pull right, three tend to pull left, but in below, left pulled in general has higher value than right pulled ???
```

```{r calculate contrast between no-partner/partner}
diffs <- list(
  db13 = post$b[,1]-post$b[,3],
  db24 = post$b[,2]-post$b[,4]
)
plot(precis(diffs))  
# what is the threshold for considering obvious difference here?
# no evidence of prosocial choice
```
```{r summarize the proportions of left pulls for each actor in each treatment and then plot against the posterior predictions}
pl <- by( d$pulled_left , list( d$actor , d$treatment ) , mean ) # didn't see it as a matrix form exhibiting pl
pl[1,]
# model will make predictions for these values
```

```{r posterior prediction check}
# long code, visit back later!
plot( NULL , xlim=c(1,28) , ylim=c(0,1) , xlab="" ,
    ylab="proportion left lever" , xaxt="n" , yaxt="n" )
axis( 2 , at=c(0,0.5,1) , labels=c(0,0.5,1) )
abline( h=0.5 , lty=2 )
for ( j in 1:7 ) abline( v=(j-1)*4+4.5 , lwd=0.5 )
for ( j in 1:7 ) text( (j-1)*4+2.5 , 1.1 , concat("actor ",j) , xpd=TRUE )
for ( j in (1:7)[-2] ) {
    lines( (j-1)*4+c(1,3) , pl[j,c(1,3)] , lwd=2 , col=rangi2 )
    lines( (j-1)*4+c(2,4) , pl[j,c(2,4)] , lwd=2 , col=rangi2 )
}
points( 1:28 , t(pl) , pch=16 , col="white" , cex=1.7 )
points( 1:28 , t(pl) , pch=c(1,1,16,16) , col=rangi2 , lwd=2 )
yoff <- 0.01
text( 1 , pl[1,1]-yoff , "R/N" , pos=1 , cex=0.8 )
text( 2 , pl[1,2]+yoff , "L/N" , pos=3 , cex=0.8 )
text( 3 , pl[1,3]-yoff , "R/P" , pos=1 , cex=0.8 )
text( 4 , pl[1,4]+yoff , "L/P" , pos=3 , cex=0.8 )
mtext( "observed proportions\n" )
```

```{r compute posterior predictions}
dat <- list( actor=rep(1:7,each=4) , treatment=rep(1:4,times=7) )
p_post <- link_ulam( m11.4 , data=dat )
p_mu <- apply( p_post , 2 , mean )
p_ci <- apply( p_post , 2 , PI )
#The model expects almost no change when adding a partner. Most of the variation in predictions comes from the actor intercepts. Handedness seems to be the big story of this experiment.
```

## build model without interaction effect as seem above no obvious interaction effect observed 
```{r build index variables split location of prosocial option and partner}
d$side <- d$prosoc_left +1 # right 1, left 2
d$cond <- d$condition+1 # no partner 1, partner 2 # add 1 to feed into ulam
```

```{r build model use index variable seperate prosocial side and partner}
dat_list2 <- list(
  pulled_left = d$pulled_left,
  actor = d$actor,
  side = d$side,
  cond = d$cond
)
m11.5 <- ulam(
  alist(
    pulled_left ~ dbinom(1,p),
    logit(p) <- a[actor] + bs[side] + bc[cond],
    a[actor] ~ dnorm(0,1.5),
    bs[side] ~ dnorm(0,0.5),
    bc[cond] ~ dnorm(0,0.5)
  ),
  data = dat_list2,chains = 4,log_lik = TRUE)
```

```{r compare model with or without seperated index variable of location and partner prensence}
compare(m11.5,m11.4,func = LOO)
```

#11.1.2 Relative shark and absolute penguin
```{r calculate proportional odds of switching from treatment 2 to 4}
post <- extract.samples(m11.4)
mean(exp(post$b[,4]-post$b[,2]))
```
#11.1.3 Aggregated binomial 
```{r calculate number of times each chimpanzee pulled the left-hand lever, for each combination of predictor values}
# data transform
data("chimpanzees")
d <- chimpanzees
d$treatment <- 1+d$prosoc_left+2*d$condition
d$side <- d$prosoc_left +1 # right 1, left 2
d$cond <- d$condition + 1 # no partner 1, partner 2
d_aggregated <- aggregate(
  d$pulled_left,
  list(treatment=d$treatment,actor=d$actor,side=d$side,cond=d$cond), sum
)
colnames(d_aggregated)[5] <- "left_pulls" #count each actor pulled left-hand lever for trials in each treatment
```

```{r build the model}
dat <- with(d_aggregated,list(
  left_pulls=left_pulls,
  treatment=treatment,
  actor=actor,
  side=side,
  cond=cond
))

m11.6 <- ulam(
  alist(
    left_pulls ~ dbinom(18,p),
    logit(p) <- a[actor]+b[treatment],
    a[actor] ~ dnorm(0,1.5), # how to decide 1.5 here
    b[treatment] ~ dnorm(0,0.5)
  ), data = dat, chains = 4, log_lik = TRUE
)
```
```{r}
compare( m11.6 , m11.4 , func=LOO )
```

```{r extract and inspect the Pareto diagnostics}
( k <- LOOPk(m11.6) )
#Larger values indicate less reliable pointwise estimates and more influential points. 
# 28 actor-treatment combinations
#values between 0.5 and 1 are worrisome, and anything above 1 is wildly untrustworthy
```

# 11.1.4 Aggregated binomial: Graduate school admissions
```{r UCB data import}
# insert variable to handle uneven trial numbers
library(rethinking)
data("UCBadmit")
d <- UCBadmit
# evaluate whether these data contain evidence of gender bias in admissions, model admission decisions using applicant gender as a predictor variable, fit binomial regression that models admit as a function of each applicant's gender
```

```{r build model}
d$gid <- ifelse(d$applicant.gender=="male",1,2)
m11.7 <- quap(
  alist(
    admit ~ dbinom(applications,p),
    logit(p) <- a[gid],
    a[gid] ~ dnorm(0,1.5)
    ),data = d
  )
precis(m11.7,depth = 2)
```

```{r compute contrast on the logit scale relative and outcome scale absolute}
post <- extract.samples(m11.7)
diff_a <- post$a[,1]-post$a[,2]
diff_p <- inv_logit(post$a[,1])-inv_logit(post$a[,2])
precis(list(diff_a=diff_a,diff_p=diff_p))
```

```{r posteior prediction for the model}
# posterior validation check
postcheck( m11.7 , n=1e4 )
# draw lines connecting points from same dept  # look into details later
d$dept_id <- rep( 1:7 , each=2 )
for ( i in 1:6 ) {
    x <- 1 + 2*(i-1)
    y1 <- d$admit[x]/d$applications[x]
    y2 <- d$admit[x+1]/d$applications[x+1]
    lines( c(x,x+1) , c(y1,y2) , col=rangi2 , lwd=2 )
    text( x+0.5 , (y1+y2)/2 + 0.05 , d$dept[x] , cex=0.8 , col=rangi2 )
}
# males and females do not apply to the same departments, and departments vary in their rates of admission
```

```{r within department model build}
d$dept_id <- rep(1:6,each=2)
m11.8 <- quap(
  alist(
    admit ~ dbinom(applications,p),
    logit(p) <- a[gid]+delta[dept_id],
    a[gid] ~ dnorm(0,1.5),
    delta[dept_id] ~ dnorm(0,1.5)
  ),data = d
)
precis(m11.8,depth = 2)
```

```{r calculate contrasts}
post <- extract.samples(m11.8)
diff_a <- post$a[,1]-post$a[,2]
diff_p <- inv_logit(post$a[,1])-inv_logit(post$a[,2])
precis(list(diff_a=diff_a,diff_p=diff_p))
```

```{r compute gender proportion in each department}
#look into detail later
pg <- sapply( 1:6 , function(k)
    d$applications[d$dept_id==k]/sum(d$applications[d$dept_id==k]) )
rownames(pg) <- c("male","female")
colnames(pg) <- unique(d$dept)
round( pg , 2 )
```

```{r}
postcheck( m11.8 , n=1e4 )
```

```{r}
# over-parameterized
pairs(m11.8)
```

##11.2 Poission Regression
```{r simulate binomial process of 1000,0.001}
y <- rbinom(1e5,1000,1/1000)
c(mean(y),var(y))
```
#11.2.1
```{r}
library(rethinking)
data(Kline)
d <- Kline
```

```{r data transformation}
d$P <- scale(log(d$population))
d$contact_id <- ifelse(d$contact=="high",2,1)
```

```{r prior predictive simulation}

```

```{r build model and approximate posterior distributin}
dat <- list(
  T = d$total_tools,
  P = d$P,
  cid = d$contact_id
)
```

```{r compare models with or w/o interactions}
m11.9 <- ulam(
  alist(
    T ~ dpois(lambda),
    log(lambda) <- a,
    a ~ dnorm(3,0.5)
  ),data = dat,chains = 4, log_lik = TRUE
)
```

```{r interaction model}
m11.10 <- ulam(
  alist(
    T ~ dpois(lambda),
    log(lambda) <- a[cid]+b[cid]*P,
    a[cid] ~ dnorm(3,0.5),
    b[cid] ~ dnorm(0,0.2)
  ),data = dat, chains = 4, log_lik = TRUE
)
```

```{r compare two models}
compare(m11.9,m11.10,func = LOO)
```
## 11.2.2 Negative binomial (gamma-Poisson) models 
```{r}

```


---
title: "Chapter11_HW"
author: "Lin Zhang"
date: "11/2/2019"
output: 
  html_document: 
    keep_md: yes
---



# 10E1. If an event has probability 0.35, what are the log-odds of this event?

```r
log(0.35/(1-0.35))
```

```
## [1] -0.6190392
```

```r
logit(.35)
```

```
## [1] -0.6190392
```
# 10E2. If an event has log-odds 3.2, what is the probability of this event?

```r
inv_logit(3.2)
```

```
## [1] 0.9608343
```

```r
exp(3.2)/(1+exp(3.2))
```

```
## [1] 0.9608343
```

#10E3. Suppose that a coefficient in a logistic regression has value 1.7. What does this imply about the proportional change in odds of the outcome?
log(odds)=a+b*x, b=1.7 
e^1.7/e^1=e^0.7, increase the probability of the event by 70%

#10M1. As explained in the chapter, binomial data can be organized in aggregated and disaggregated forms, without any impact on inference. But the likelihood of the data does change when the data are converted between the two formats. Can you explain why?
?? 
extra parameter, extra factor

# Week6_Problem_1 NWO fund

```r
data("NWOGrants")
d <- NWOGrants
str(d)
```

```
## 'data.frame':	18 obs. of  4 variables:
##  $ discipline  : Factor w/ 9 levels "Chemical sciences",..: 1 1 6 6 7 7 3 3 9 9 ...
##  $ gender      : Factor w/ 2 levels "f","m": 2 1 2 1 2 1 2 1 2 1 ...
##  $ applications: int  83 39 135 39 67 9 230 166 189 62 ...
##  $ awards      : int  22 10 26 9 18 2 33 32 30 13 ...
```


```r
#build model for total effect of gender on grant awards
d$gid <- ifelse(d$gender=="m",1,2)
m1.1 <- quap(
  alist(
    awards ~ dbinom(applications,p),
    logit(p) <- a[gid],
    a[gid] ~ dnorm(0,1.5)
    ),data = d
  )
precis(m1.1,depth = 2)
```

```
##           mean         sd      5.5%     94.5%
## a[1] -1.531418 0.06462435 -1.634700 -1.428136
## a[2] -1.737428 0.08121368 -1.867223 -1.607633
```


```r
#compute contrast on the logit scale relative and outcome scale absolute
post <- extract.samples(m1.1)
diff_a <- post$a[,1]-post$a[,2]
diff_p <- inv_logit(post$a[,1])-inv_logit(post$a[,2])
precis(list(diff_a=diff_a,diff_p=diff_p))
```

```
##              mean         sd        5.5%      94.5%
## diff_a 0.20341300 0.10351755 0.037063076 0.36868710
## diff_p 0.02767297 0.01397465 0.005126048 0.04960957
##                                                                                       histogram
## diff_a                         <U+2581><U+2581><U+2582><U+2587><U+2587><U+2583><U+2581><U+2581>
## diff_p <U+2581><U+2581><U+2581><U+2582><U+2585><U+2587><U+2587><U+2583><U+2581><U+2581><U+2581>
```

```r
# in general, across discipline, male is 0.03 higher in probablity space to get awarded
```


```r
library(dagitty)
dag1 <- dagitty("dag{
                G->D
                D->A
                G->A
                }"
)
coordinates(dag1) <- list(x=c(G=0,D=1,A=2),y=c(G=0,A=0,D=1))
drawdag(dag1)
```

![](Chapter11_HW_files/figure-html/DAG-1.png)<!-- -->


```r
#build model for total effect of gender on grant awards
d$dis_id <- rep(1:9,each=2)
m1.2 <- quap(
  alist(
    awards ~ dbinom(applications,p),
    logit(p) <- a[gid]+delta[dis_id],
    a[gid] ~ dnorm(0,1.5),
    delta[dis_id] ~ dnorm(0,1.5)
  ),data = d
)
precis(m1.2,depth = 2)
```

```
##                mean        sd       5.5%      94.5%
## a[1]     -1.1578401 0.4565098 -1.8874309 -0.4282494
## a[2]     -1.2955002 0.4597880 -2.0303302 -0.5606702
## delta[1]  0.1637017 0.4908967 -0.6208461  0.9482495
## delta[2] -0.1884150 0.4856753 -0.9646180  0.5877880
## delta[3]  0.1398275 0.5114927 -0.6776366  0.9572916
## delta[4] -0.4103747 0.4709093 -1.1629787  0.3422293
## delta[5] -0.3809051 0.4792306 -1.1468082  0.3849980
## delta[6] -0.4466022 0.4895118 -1.2289366  0.3357321
## delta[7] -0.1755191 0.4742532 -0.9334674  0.5824292
## delta[8] -0.6369930 0.4640407 -1.3786196  0.1046336
## delta[9] -0.5156652 0.4687250 -1.2647782  0.2334478
```


```r
post <- extract.samples(m1.2)
diff_a <- post$a[,1]-post$a[,2]
diff_p <- inv_logit(post$a[,1])-inv_logit(post$a[,2])
precis(list(diff_a=diff_a,diff_p=diff_p))
```

```
##              mean         sd         5.5%      94.5%
## diff_a 0.13835626 0.10746815 -0.033225672 0.30895199
## diff_p 0.02399182 0.01982271 -0.005678494 0.05640977
##                                                                       histogram
## diff_a <U+2581><U+2581><U+2582><U+2585><U+2587><U+2585><U+2581><U+2581><U+2581>
## diff_p <U+2581><U+2581><U+2582><U+2587><U+2587><U+2583><U+2581><U+2581><U+2581>
```

```r
# condition on  discipline, male is 0.02 higher in probablity space to get awarded
```

# Consider an unobserved confound

```r
library(dagitty)
dag1 <- dagitty("dag{
                G->D
                D->A
                G->A
                U->D
                U->A
                }"
)
coordinates(dag1) <- list(x=c(G=0,D=0,A=1,U=1),y=c(G=0,A=0,D=1,U=1))
drawdag(dag1)
```

![](Chapter11_HW_files/figure-html/DAG2-1.png)<!-- -->


```r
# conditional on discipline causes gender and unobserved confound(such as career stages) to be correlated, the collider is open
# it does not provide an un-confounded estimate of the direct path from gender to an award, as the backdoor is open through the unobserved confound
# simulate?
# but for a regression conditioning on both gender adn discipline to suggest zero influence??
```

# Chapter 11.2 problems

##10E4. 
Why do Poisson regressions sometimes require the use of an offset? Provide an example.

```r
# Possion regressions reflect count of an event over a fixed interval, sometimes two datasets have different intervals, this would require a offset. The example in the textbook as in the monastery, whether production is counted per day or per week. Other examples include recombination event across genome, per 100kb or per 1MB, as well as SNP density.
```

## 10M2. 
If a coefficient in a Poisson regression has value 1.7, what does this imply about the change in the outcome?

```r
# for every unit change in the predictor there will be a 5.47-fold (e^1.7) increase in the number of counts
# a^n*a^m=a^(n+m)
# exp(a+1.7(x+1)) = exp(1.7)exp(a+1.7x)
exp(1.7)
```

```
## [1] 5.473947
```

## 10M3. 
Explain why the logit link is appropriate for a binomial generalized linear model.

```r
# binomial generalized linear model uses a binomial distribution to model count in a binary option with the parameter being probability of one option in each single trial, logit link would transfer a linear model to a probability scale and constrain the parameter between 0 and 1
```

## 10M4. 
Explain why the log link is appropriate for a Poisson generalized linear model.

```r
#Poisson generalized linear model use Poisson distribution to model count within a fixed interval with low rate, the count has no upper limit. 
# a log link map linear model to positive reals
```

# Chapter 11.3 problems
##10H3 
The data contained in library(MASS);data(eagles) are records of salmon pirating attempts by Bald Eagles in Washington State. See ?eagles for details. While one eagle feeds, sometimes another will swoop in and try to steal the salmon from it. Call the feeding eagle the “victim” and
the thief the “pirate.” Use the available data to build a binomial GLM of successful pirating attempts.


```r
library(rethinking)
library(MASS)
```

```
## 
## Attaching package: 'MASS'
```

```
## The following object is masked from 'package:dplyr':
## 
##     select
```

```r
data("eagles")
d <- eagles
```


```r
summary(is.na(d))
```

```
##      y               n               P               A          
##  Mode :logical   Mode :logical   Mode :logical   Mode :logical  
##  FALSE:8         FALSE:8         FALSE:8         FALSE:8        
##      V          
##  Mode :logical  
##  FALSE:8
```

```r
d$Aid <- as.numeric(ifelse(d$A=="A",1,0))
d$Pid <- as.numeric(ifelse(d$P=="L",1,0))
d$Vid <- as.numeric(ifelse(d$V=="L",1,0))
```
### (a)
Consider the following model:
yi ∼ Binomial(ni; pi)
log pi
1 − pi= α + βPPi + βVVi + βAAi
α ∼ Normal(0; 10)
βP ∼ Normal(0; 5)
βV ∼ Normal(0; 5)
βA ∼ Normal(0; 5)
where y is the number of successful attempts, n is the total number of attempts, P is a dummy variable
indicating whether or not the pirate had large body size, V is a dummy variable indicating whether
or not the victim had large body size, and finally A is a dummy variable indicating whether or not the
pirate was an adult. Fit the model above to the eagles data, using both map and map2stan. Is the
quadratic approximation okay?

```r
m11.3.1 <- quap(
  alist(
    y ~ dbinom(n,p),
    logit(p) <- a+bp*Pid+bv*Vid+bA*Aid,
    # why not use 6 types of treatments like R code 11.7 # why not use index variable
    a ~ dnorm(0,1.5),  # dnorm(0,10) would pile up probability mass near zero and one
    bp ~ dnorm(0,5),
    bv ~ dnorm(0,5),
    bA ~ dnorm(0,5)
  ),data=d
)
```


```r
prior <- extract.prior(m11.3.1,n=1e4)
p <- inv_logit(prior$a)
dens(p,adj = 0.5)
```

![](Chapter11_HW_files/figure-html/sample from prior-1.png)<!-- -->


```r
plot(precis(m11.3.1,depth = 2)) # quap is alright with a regularized prior
```

![](Chapter11_HW_files/figure-html/unnamed-chunk-11-1.png)<!-- -->

```r
precis(m11.3.1,depth = 2)
```

```
##          mean        sd       5.5%     94.5%
## a   0.4977641 0.6020250 -0.4643880  1.459916
## bp  4.2646081 0.8948486  2.8344672  5.694749
## bv -4.5360873 0.9434080 -6.0438355 -3.028339
## bA  1.1235951 0.5211947  0.2906252  1.956565
```


```r
dat <- list(y=d$y,n=d$n,Pid=d$Pid,Vid=d$Vid,Aid=d$Aid)
m11.3.2 <- ulam(
  alist(
    y ~ dbinom(n,p),
    logit(p) <- a+bp*Pid+bv*Vid+bA*Aid, # suppose there is no intercation between Pid, Vid or Aid
    # why not use 6 types of treatments like R code 11.7 # why not use index variable
    a ~ dnorm(0,1.5),
    bp ~ dnorm(0,1.5),
    bv ~ dnorm(0,1.5),
    bA ~ dnorm(0,1.5)
  ),data=dat, chains = 4,cores = 4, log_lik = TRUE
)
```


```r
prior <- extract.prior(m11.3.2,n=1e4)
```

```
## 
## SAMPLING FOR MODEL '6b839b7f8a2735720d95d36c1a29387d' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)
## Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)
## Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)
## Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)
## Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)
## Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)
## Chain 1: Iteration: 10001 / 20000 [ 50%]  (Sampling)
## Chain 1: Iteration: 12000 / 20000 [ 60%]  (Sampling)
## Chain 1: Iteration: 14000 / 20000 [ 70%]  (Sampling)
## Chain 1: Iteration: 16000 / 20000 [ 80%]  (Sampling)
## Chain 1: Iteration: 18000 / 20000 [ 90%]  (Sampling)
## Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 1.039 seconds (Warm-up)
## Chain 1:                1.029 seconds (Sampling)
## Chain 1:                2.068 seconds (Total)
## Chain 1:
```

```r
p <- inv_logit(prior$a)
dens(p,adj = 0.1)
```

![](Chapter11_HW_files/figure-html/sample from prior of intercept-1.png)<!-- -->


```r
# how to calculate prior for all of the coefficients ????
prior <- extract.prior(m11.3.2,n=1e4)
```

```
## recompiling to avoid crashing R session
```

```
## 
## SAMPLING FOR MODEL '6b839b7f8a2735720d95d36c1a29387d' NOW (CHAIN 1).
## Chain 1: 
## Chain 1: Gradient evaluation took 0.001 seconds
## Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 10 seconds.
## Chain 1: Adjust your expectations accordingly!
## Chain 1: 
## Chain 1: 
## Chain 1: Iteration:     1 / 20000 [  0%]  (Warmup)
## Chain 1: Iteration:  2000 / 20000 [ 10%]  (Warmup)
## Chain 1: Iteration:  4000 / 20000 [ 20%]  (Warmup)
## Chain 1: Iteration:  6000 / 20000 [ 30%]  (Warmup)
## Chain 1: Iteration:  8000 / 20000 [ 40%]  (Warmup)
## Chain 1: Iteration: 10000 / 20000 [ 50%]  (Warmup)
## Chain 1: Iteration: 10001 / 20000 [ 50%]  (Sampling)
## Chain 1: Iteration: 12000 / 20000 [ 60%]  (Sampling)
## Chain 1: Iteration: 14000 / 20000 [ 70%]  (Sampling)
## Chain 1: Iteration: 16000 / 20000 [ 80%]  (Sampling)
## Chain 1: Iteration: 18000 / 20000 [ 90%]  (Sampling)
## Chain 1: Iteration: 20000 / 20000 [100%]  (Sampling)
## Chain 1: 
## Chain 1:  Elapsed Time: 4.131 seconds (Warm-up)
## Chain 1:                0.669 seconds (Sampling)
## Chain 1:                4.8 seconds (Total)
## Chain 1:
```

```r
p <- inv_logit(prior$bp)
dens(p,adj = 0.1)
```

![](Chapter11_HW_files/figure-html/unnamed-chunk-12-1.png)<!-- -->

```r
k <- LOOPk(m11.3.2)
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```r
dat <- list(dat,c(1:8))
#plot(dat$)
```


```r
plot(precis(m11.3.2,depth = 2))
```

![](Chapter11_HW_files/figure-html/parameter estimation-1.png)<!-- -->

```r
precis(m11.3.2,depth = 2)
```

```
##          mean        sd       5.5%     94.5%     n_eff      Rhat
## a   0.4466586 0.5794977 -0.4620347  1.420428 1046.3607 1.0035765
## bp  3.2778019 0.5469272  2.4312439  4.184209 1121.6770 1.0000008
## bv -3.4850357 0.6083546 -4.4896464 -2.544348  992.3684 1.0005964
## bA  1.0340951 0.4918147  0.2860774  1.836452 1267.9423 0.9995117
```

```r
# parameter estimation by the quap and ulam is very similar
```

```r
#? how to deal with this
(k <- LOOPk(m11.3.2))
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```
## [1] 0.9487216 0.3570850 0.9215727 0.3159564 0.6371484 0.9762458 0.6936566
## [8] 0.4763020
```


```r
post <- extract.samples(m11.3.2)
(inv_logit(post$a))
```

```
##    [1] 0.7358789 0.8338192 0.7219511 0.7486850 0.4667765 0.8888800
##    [7] 0.7339392 0.3171039 0.5435042 0.4928043 0.7128184 0.7336806
##   [13] 0.6745067 0.5229961 0.2421839 0.4208169 0.7083530 0.8440409
##   [19] 0.4856620 0.5390768 0.6715782 0.8128773 0.4373691 0.7090136
##   [25] 0.5391160 0.5149514 0.8616897 0.7208236 0.6550822 0.5180456
##   [31] 0.5010057 0.4156982 0.5653727 0.4457266 0.4884459 0.7814289
##   [37] 0.7441629 0.7885452 0.6077712 0.7743116 0.5268980 0.5962941
##   [43] 0.6235661 0.7148565 0.7912193 0.7671533 0.4642765 0.8028485
##   [49] 0.6923178 0.3781919 0.6102678 0.6235973 0.4031692 0.5786387
##   [55] 0.4821941 0.6293467 0.5326160 0.7937934 0.6808831 0.6706438
##   [61] 0.7428629 0.4599227 0.5957236 0.6508157 0.8863333 0.4811846
##   [67] 0.5198175 0.5890921 0.5977037 0.8425117 0.5778163 0.7540193
##   [73] 0.5049677 0.4841826 0.7108616 0.7970804 0.7923568 0.4684020
##   [79] 0.6104445 0.4112996 0.3706600 0.6021972 0.6419939 0.5884627
##   [85] 0.6969994 0.6994746 0.7353735 0.8443566 0.8869870 0.7056151
##   [91] 0.5829134 0.5114616 0.5805220 0.3602392 0.4660581 0.5632841
##   [97] 0.8501090 0.6798109 0.6291716 0.5269359 0.5390206 0.4368742
##  [103] 0.4722297 0.6419245 0.5998849 0.4588756 0.7323369 0.5426592
##  [109] 0.5786138 0.6447957 0.5549011 0.6918858 0.7483050 0.6372728
##  [115] 0.4621171 0.3088686 0.5486275 0.6672538 0.8444723 0.6396505
##  [121] 0.5089118 0.8326211 0.8473094 0.3952494 0.6532138 0.6645074
##  [127] 0.4501959 0.5742531 0.6645375 0.7072109 0.3806244 0.6981924
##  [133] 0.7836988 0.2577963 0.7357116 0.5441725 0.7992025 0.7443476
##  [139] 0.5418244 0.2282951 0.7891627 0.7866286 0.6401013 0.6104789
##  [145] 0.6227932 0.4060988 0.6148997 0.5909455 0.6568838 0.5382997
##  [151] 0.5508631 0.5052329 0.5291200 0.5527033 0.5995991 0.7259862
##  [157] 0.6226881 0.8215310 0.7513129 0.6345315 0.4437943 0.7101269
##  [163] 0.5801220 0.6046164 0.5544476 0.7603717 0.7463065 0.7733868
##  [169] 0.7535294 0.4455251 0.5755876 0.4812374 0.7335988 0.8239966
##  [175] 0.4230802 0.6129396 0.6359030 0.4929639 0.5367779 0.7704418
##  [181] 0.6513677 0.6268265 0.5231959 0.5348109 0.4058845 0.8039104
##  [187] 0.8063050 0.5680968 0.4892648 0.6221330 0.8257859 0.7742761
##  [193] 0.4771475 0.7189239 0.8137295 0.4733340 0.4773380 0.8379217
##  [199] 0.8973349 0.5672043 0.3809097 0.4748937 0.4858756 0.7051781
##  [205] 0.7652644 0.6019146 0.3686881 0.8111179 0.8051379 0.4254297
##  [211] 0.3917765 0.8853321 0.4265215 0.7162572 0.6545071 0.4565364
##  [217] 0.7832815 0.6759370 0.5560020 0.5065790 0.2745363 0.5317678
##  [223] 0.7925665 0.6058693 0.6282510 0.3713601 0.8517980 0.5996596
##  [229] 0.7197671 0.6202702 0.6114360 0.4251425 0.5831769 0.8827855
##  [235] 0.5091820 0.5958290 0.5431199 0.4678560 0.6677372 0.6631505
##  [241] 0.8732090 0.5958901 0.6880292 0.5668196 0.4648710 0.4448617
##  [247] 0.4632475 0.6229213 0.4096557 0.6629402 0.6690661 0.4959406
##  [253] 0.6609109 0.2661856 0.4219359 0.3289109 0.5668544 0.6494976
##  [259] 0.5309520 0.5548621 0.7489763 0.6825872 0.5381267 0.4049366
##  [265] 0.8598614 0.6242338 0.4703472 0.6285649 0.3768869 0.3951143
##  [271] 0.5773106 0.6338813 0.3895206 0.5266131 0.7131552 0.5993548
##  [277] 0.5829422 0.5670523 0.7558558 0.7236655 0.3612692 0.6008087
##  [283] 0.6183890 0.7164534 0.6483037 0.6836199 0.6252877 0.8071360
##  [289] 0.4254945 0.6782407 0.4284281 0.4925758 0.4615200 0.5491069
##  [295] 0.4612999 0.5824073 0.7003850 0.6779287 0.7310352 0.6333969
##  [301] 0.4568452 0.6785179 0.6599098 0.8249345 0.5766090 0.5552000
##  [307] 0.7216709 0.7870139 0.3207005 0.6028679 0.4359048 0.5054102
##  [313] 0.6059519 0.6766791 0.7042468 0.5951426 0.7409812 0.8308719
##  [319] 0.7261969 0.6336644 0.8811721 0.6016074 0.6636675 0.5551167
##  [325] 0.6270917 0.7584770 0.4085665 0.5078358 0.4318437 0.6091895
##  [331] 0.3452397 0.5481513 0.4670535 0.5382172 0.6745456 0.7035082
##  [337] 0.5571237 0.4537465 0.8116610 0.5272797 0.8147367 0.4761229
##  [343] 0.5296721 0.4509669 0.4806749 0.6002848 0.7117440 0.6467251
##  [349] 0.7106168 0.6503234 0.8219766 0.7875132 0.6768712 0.6762468
##  [355] 0.8359864 0.4929041 0.6266208 0.4196305 0.7107070 0.8532799
##  [361] 0.7040208 0.7169301 0.5097888 0.7508045 0.6478976 0.5212666
##  [367] 0.5786682 0.3912661 0.7147027 0.5102998 0.7201540 0.6831835
##  [373] 0.5907125 0.6171052 0.6910007 0.6063602 0.7033827 0.5269017
##  [379] 0.6413882 0.6491832 0.5406042 0.3596235 0.7457025 0.6195782
##  [385] 0.8174638 0.4257293 0.5071472 0.5002680 0.6279549 0.5962960
##  [391] 0.4619716 0.7072018 0.5966248 0.6827390 0.6037613 0.6369559
##  [397] 0.5600566 0.5944657 0.5713077 0.5801305 0.6538190 0.8123826
##  [403] 0.5414770 0.9070850 0.6244134 0.5004886 0.4081058 0.7435578
##  [409] 0.7704037 0.7049028 0.7087542 0.7308792 0.5705653 0.4787246
##  [415] 0.6368762 0.6626705 0.6887315 0.6058950 0.7460892 0.5972477
##  [421] 0.8163146 0.6057133 0.7433392 0.5368287 0.6096445 0.8349642
##  [427] 0.5805286 0.6348693 0.3932134 0.6847556 0.4994151 0.4544649
##  [433] 0.5605323 0.6208612 0.4675125 0.8391334 0.5108026 0.7055875
##  [439] 0.6296443 0.6845290 0.7202611 0.6284275 0.8625580 0.6122114
##  [445] 0.8863237 0.8657707 0.6674894 0.7148054 0.6068805 0.8147408
##  [451] 0.6695822 0.4565945 0.7351193 0.5369229 0.7177412 0.6605709
##  [457] 0.6577499 0.3689690 0.5834738 0.5053515 0.4666130 0.5290827
##  [463] 0.8035347 0.7167647 0.7372480 0.6509740 0.5680693 0.3860178
##  [469] 0.7473442 0.6221622 0.5171192 0.6506912 0.3495148 0.8511468
##  [475] 0.5559062 0.5260791 0.5123053 0.5781528 0.5023203 0.6786428
##  [481] 0.4393942 0.4184266 0.8624572 0.7384633 0.4679804 0.4929989
##  [487] 0.7271068 0.4131839 0.6152046 0.3692614 0.7468686 0.8144402
##  [493] 0.3499094 0.3710427 0.5511394 0.9044015 0.6692591 0.5639628
##  [499] 0.5386279 0.6810481 0.5898592 0.4636389 0.7416437 0.8224888
##  [505] 0.4359561 0.6062246 0.6429587 0.5329210 0.7261220 0.6644761
##  [511] 0.5703996 0.7311037 0.5912672 0.4357109 0.6395613 0.7736838
##  [517] 0.7031659 0.7087280 0.6609117 0.6902206 0.6277112 0.6051853
##  [523] 0.6936434 0.4545108 0.7345221 0.5323628 0.6133287 0.6119319
##  [529] 0.6103848 0.7368241 0.5187649 0.5462628 0.8239492 0.4143510
##  [535] 0.7308199 0.5021927 0.4393064 0.5064353 0.7021193 0.6590226
##  [541] 0.5411120 0.6072000 0.5256312 0.7145274 0.4898614 0.6423289
##  [547] 0.4961154 0.4028862 0.6991215 0.6015843 0.4898297 0.5238673
##  [553] 0.5334740 0.6027492 0.5731862 0.7811034 0.4115449 0.6566041
##  [559] 0.5133481 0.5524427 0.6998912 0.5182225 0.5437227 0.6823136
##  [565] 0.7539716 0.7186778 0.5193382 0.5302540 0.6837019 0.7034947
##  [571] 0.7550230 0.6495201 0.7130839 0.4620601 0.6103639 0.4722831
##  [577] 0.5840080 0.7719000 0.5438510 0.5363630 0.5753248 0.6342027
##  [583] 0.7280626 0.7028078 0.6170772 0.6062983 0.5767666 0.5969009
##  [589] 0.5035430 0.4331233 0.5378520 0.6304196 0.6728161 0.4613903
##  [595] 0.4775812 0.5490294 0.5542238 0.7256751 0.6581048 0.6620701
##  [601] 0.5825965 0.8122958 0.4857941 0.5315793 0.7042617 0.3167375
##  [607] 0.4807139 0.7083062 0.6402633 0.3891215 0.6766402 0.4201861
##  [613] 0.5718300 0.4826499 0.8623595 0.3700189 0.7047898 0.6776123
##  [619] 0.5816578 0.7199450 0.5183494 0.8630110 0.3797524 0.5795476
##  [625] 0.6983955 0.6889174 0.7376860 0.5605216 0.4367333 0.3622362
##  [631] 0.6032265 0.6494170 0.3260600 0.4029045 0.8490730 0.7474869
##  [637] 0.4468974 0.6291946 0.6785455 0.7925052 0.3976954 0.6228162
##  [643] 0.6974095 0.6561385 0.6881049 0.5433243 0.3032771 0.6592702
##  [649] 0.6844283 0.4062562 0.6255163 0.6175159 0.7221600 0.6642093
##  [655] 0.7370010 0.6256175 0.6901736 0.5041362 0.4796184 0.5785199
##  [661] 0.3074661 0.5790908 0.6773789 0.6666163 0.6636256 0.6294262
##  [667] 0.6081812 0.5805790 0.5579958 0.4789670 0.7800278 0.7363320
##  [673] 0.7026392 0.7163651 0.5564166 0.4490269 0.4243274 0.6728161
##  [679] 0.5750678 0.6828215 0.6077651 0.7057876 0.5212227 0.6861489
##  [685] 0.7017048 0.5766436 0.4228353 0.3839524 0.6070215 0.8575126
##  [691] 0.7874456 0.5410637 0.8778060 0.4835745 0.8726982 0.6349325
##  [697] 0.4689102 0.5441659 0.6064611 0.6352444 0.7912064 0.6778296
##  [703] 0.6648812 0.5961243 0.8228404 0.5948047 0.7530000 0.8487092
##  [709] 0.7467517 0.7315532 0.8598782 0.3890975 0.7492384 0.5371164
##  [715] 0.7957734 0.4147739 0.6409591 0.4676180 0.5901165 0.6868785
##  [721] 0.4802588 0.6587627 0.7830725 0.6791419 0.6783292 0.5457920
##  [727] 0.7552007 0.4928355 0.5266717 0.7226356 0.8003659 0.5980964
##  [733] 0.7321110 0.6699064 0.5383879 0.3826796 0.4309029 0.3536385
##  [739] 0.4435110 0.6505473 0.6635109 0.4971471 0.4597879 0.7647044
##  [745] 0.6066485 0.7431052 0.5721039 0.5246970 0.5868817 0.6442515
##  [751] 0.7037476 0.2228667 0.5286582 0.6515176 0.5577261 0.7386441
##  [757] 0.3410997 0.6042602 0.8218954 0.7034296 0.6373367 0.5871858
##  [763] 0.5375217 0.7531079 0.3666818 0.8176195 0.7789437 0.7577571
##  [769] 0.4006270 0.6381764 0.5900664 0.8729852 0.5230413 0.5648801
##  [775] 0.6541796 0.5513979 0.7183541 0.4936844 0.7213223 0.7230273
##  [781] 0.4051406 0.7428466 0.5822864 0.6509348 0.6073216 0.7458714
##  [787] 0.7014168 0.7470628 0.4795119 0.7616480 0.5380973 0.5904018
##  [793] 0.6723516 0.6601437 0.5934545 0.7867528 0.5041441 0.6861283
##  [799] 0.6357455 0.2793408 0.4833849 0.7793696 0.8391663 0.6053410
##  [805] 0.6694984 0.6980315 0.4078218 0.7533749 0.5143206 0.5359052
##  [811] 0.7135791 0.5443791 0.2606675 0.3674258 0.6758171 0.4476079
##  [817] 0.7455081 0.5003696 0.5885999 0.6832133 0.6505469 0.5520762
##  [823] 0.5470135 0.7727150 0.3885433 0.4480814 0.6117913 0.4635837
##  [829] 0.6243561 0.6600467 0.8521053 0.4493553 0.7368900 0.6932724
##  [835] 0.3730006 0.5885205 0.4144130 0.4784713 0.6475604 0.7589931
##  [841] 0.6329284 0.7071620 0.4070443 0.8544539 0.7047112 0.6837932
##  [847] 0.2865703 0.5528883 0.3766148 0.4351604 0.7578829 0.5390118
##  [853] 0.6591014 0.6388075 0.3916956 0.6436199 0.5427958 0.6540203
##  [859] 0.6010263 0.7100783 0.7775383 0.6809425 0.7233137 0.6434851
##  [865] 0.6897823 0.5138798 0.7099909 0.5204422 0.6545210 0.6812145
##  [871] 0.6196050 0.4766216 0.8342681 0.5701127 0.6874245 0.6566041
##  [877] 0.4142020 0.8203173 0.4834625 0.5932004 0.5903768 0.6935714
##  [883] 0.6227007 0.6669930 0.4493937 0.6322539 0.7001065 0.7740998
##  [889] 0.5462962 0.4726214 0.5032520 0.5744868 0.6747855 0.6383108
##  [895] 0.4595790 0.5843141 0.6461756 0.6354781 0.3390007 0.6159195
##  [901] 0.6430783 0.4596883 0.4222582 0.5510758 0.4862640 0.6504108
##  [907] 0.7739147 0.5814412 0.6049022 0.6754387 0.7403501 0.5790689
##  [913] 0.6645696 0.4523929 0.7137477 0.6448818 0.3815520 0.7198193
##  [919] 0.5630275 0.6775128 0.8043053 0.3543070 0.5857166 0.6484870
##  [925] 0.4366415 0.5895015 0.3580313 0.8845310 0.6766462 0.4092568
##  [931] 0.8505425 0.6070297 0.6648767 0.6508851 0.5865024 0.5495239
##  [937] 0.5303188 0.5975240 0.6205426 0.4808480 0.5160075 0.4482340
##  [943] 0.7152177 0.4180130 0.2676099 0.6874754 0.5296268 0.6534155
##  [949] 0.4859319 0.7684955 0.6019703 0.4412980 0.4457493 0.6375545
##  [955] 0.6505139 0.7839300 0.4670009 0.5646034 0.3966864 0.7060874
##  [961] 0.6631172 0.3865315 0.4737643 0.4016621 0.6582497 0.7277238
##  [967] 0.7758197 0.6668647 0.6288895 0.5186274 0.6652781 0.6493033
##  [973] 0.6274572 0.6038909 0.3793631 0.6803296 0.7179431 0.6055491
##  [979] 0.4917278 0.5353972 0.7112668 0.5312371 0.6207128 0.5587757
##  [985] 0.7849804 0.6333428 0.6232888 0.6100608 0.6817517 0.6478437
##  [991] 0.7680065 0.4983819 0.7783067 0.6221107 0.3398423 0.3999350
##  [997] 0.6322984 0.4720680 0.4955522 0.6043301 0.5980590 0.4530149
## [1003] 0.4565043 0.7359169 0.4990197 0.5870415 0.4732103 0.6729585
## [1009] 0.6195314 0.6423416 0.7219978 0.5808771 0.6638122 0.5177506
## [1015] 0.6234689 0.6557393 0.3959595 0.6725441 0.6996774 0.6940557
## [1021] 0.4058237 0.4415029 0.4474354 0.6748624 0.6939066 0.5091670
## [1027] 0.5526392 0.4324689 0.5301827 0.7054071 0.6656103 0.5004500
## [1033] 0.5041065 0.6597999 0.6916277 0.6445847 0.7693578 0.6958734
## [1039] 0.7399350 0.6435615 0.8638381 0.5476866 0.6797442 0.4948440
## [1045] 0.5967877 0.8216162 0.5225222 0.4685180 0.5889795 0.7248484
## [1051] 0.5022324 0.6718233 0.7983786 0.5550756 0.4669045 0.3798817
## [1057] 0.4553115 0.4231659 0.6327223 0.6847921 0.7642200 0.6351455
## [1063] 0.4509177 0.6364095 0.6037958 0.6994059 0.6066035 0.4222591
## [1069] 0.6277954 0.5546351 0.8127292 0.7350822 0.5904219 0.5491839
## [1075] 0.4923379 0.5145219 0.6699389 0.6783184 0.6215598 0.6594647
## [1081] 0.6134097 0.4351054 0.4649153 0.6083007 0.5804996 0.5495015
## [1087] 0.7592358 0.6456332 0.6219582 0.7121252 0.3913138 0.3412879
## [1093] 0.6400892 0.5782476 0.7069205 0.5012390 0.6772069 0.5913595
## [1099] 0.5197341 0.6871938 0.7417782 0.4216443 0.4896204 0.4727822
## [1105] 0.7311084 0.6673172 0.6480803 0.5060205 0.4862482 0.5454846
## [1111] 0.4738542 0.6261682 0.5973802 0.6213916 0.4360141 0.7939625
## [1117] 0.6050220 0.5808078 0.8339113 0.5846808 0.5502916 0.4575797
## [1123] 0.7839637 0.4317278 0.6855028 0.7323190 0.5758071 0.5157320
## [1129] 0.6311171 0.5675104 0.5176266 0.5339371 0.3435647 0.6111778
## [1135] 0.6525693 0.7216721 0.4824032 0.6488932 0.5755404 0.5793943
## [1141] 0.6884749 0.7610881 0.6187123 0.6623117 0.4911526 0.4946698
## [1147] 0.8920952 0.5459967 0.5982969 0.6137040 0.6135256 0.6052876
## [1153] 0.4639622 0.6517436 0.5779080 0.5549327 0.5350162 0.6279093
## [1159] 0.8132353 0.5965975 0.6076702 0.7522549 0.3787379 0.3800624
## [1165] 0.7138345 0.4300107 0.6540696 0.7367750 0.6412824 0.6821945
## [1171] 0.7262325 0.5306998 0.7371624 0.7663824 0.6322605 0.4717611
## [1177] 0.7023716 0.4499556 0.7177878 0.6551219 0.6943553 0.5652842
## [1183] 0.7265851 0.6173450 0.5610042 0.6014520 0.6509846 0.6014836
## [1189] 0.7455902 0.4539323 0.4221336 0.3413866 0.3015437 0.6217585
## [1195] 0.6420473 0.5901970 0.6253853 0.3831401 0.3337801 0.5465774
## [1201] 0.4404156 0.7477813 0.7813409 0.8327701 0.3597438 0.6932325
## [1207] 0.7025297 0.7126832 0.6870469 0.4510176 0.4575359 0.5430020
## [1213] 0.7124214 0.4686577 0.5654853 0.5833289 0.6759895 0.4782818
## [1219] 0.6242396 0.7727500 0.6720086 0.6155702 0.5189627 0.3198014
## [1225] 0.7816570 0.6006615 0.3606902 0.6425971 0.4670086 0.5736661
## [1231] 0.5908178 0.4381454 0.2067508 0.6439623 0.5099396 0.5204909
## [1237] 0.4267659 0.5111012 0.7267012 0.7469409 0.4033285 0.6778154
## [1243] 0.6305373 0.5435261 0.5025599 0.3283701 0.5273272 0.4236413
## [1249] 0.4859744 0.5219406 0.6954454 0.6667013 0.4858610 0.3640240
## [1255] 0.7428306 0.4547513 0.6430366 0.6172999 0.5592954 0.4455110
## [1261] 0.4489913 0.7186411 0.8696075 0.6219891 0.4230999 0.5616400
## [1267] 0.7025870 0.6806565 0.6739162 0.7292314 0.6465115 0.7218940
## [1273] 0.4921576 0.6250152 0.6881836 0.6303016 0.7273155 0.5231091
## [1279] 0.4773151 0.8423635 0.7465766 0.6194267 0.5998623 0.6073094
## [1285] 0.7844170 0.7059658 0.7476807 0.8132465 0.4070283 0.5834935
## [1291] 0.5707963 0.6139756 0.5614514 0.6212256 0.6959769 0.5590396
## [1297] 0.7305416 0.5511252 0.6539231 0.7581477 0.6301275 0.4412661
## [1303] 0.6333999 0.6079281 0.5766490 0.5575031 0.5623628 0.6220986
## [1309] 0.6441740 0.6001581 0.5910114 0.7157502 0.5976046 0.4810367
## [1315] 0.6477824 0.4012107 0.5674223 0.5957359 0.5710937 0.6886217
## [1321] 0.3578158 0.5991406 0.6602786 0.5335339 0.7134847 0.6143006
## [1327] 0.5177153 0.5453209 0.5022785 0.6406898 0.8023623 0.6725462
## [1333] 0.6860252 0.6207588 0.5178125 0.6126992 0.6751114 0.5607843
## [1339] 0.4480854 0.4552193 0.6194922 0.4856913 0.8622593 0.5778907
## [1345] 0.5411450 0.5835839 0.6006175 0.5750849 0.7549732 0.7858370
## [1351] 0.8205111 0.6977198 0.5530131 0.4683786 0.8379596 0.5929718
## [1357] 0.4562990 0.5390094 0.7051246 0.6091799 0.5215866 0.6979620
## [1363] 0.5946686 0.7666698 0.4276808 0.6295647 0.5843448 0.5105949
## [1369] 0.6172368 0.5173700 0.6927526 0.6314263 0.4015468 0.5498020
## [1375] 0.6827321 0.5700974 0.5434452 0.6412575 0.5725209 0.4917459
## [1381] 0.6515751 0.4923327 0.6604050 0.4964822 0.8320859 0.6352351
## [1387] 0.8637581 0.5939667 0.3853863 0.5546514 0.6196421 0.6817460
## [1393] 0.5413111 0.6763211 0.5519692 0.5161157 0.7382372 0.7356910
## [1399] 0.6122765 0.6838673 0.6715316 0.5384540 0.5861308 0.3511750
## [1405] 0.5870737 0.7760132 0.4882166 0.4059900 0.6995868 0.6145079
## [1411] 0.6558780 0.5220396 0.3858928 0.4985900 0.8145168 0.6487111
## [1417] 0.5500266 0.6242802 0.3664441 0.6943414 0.5281056 0.6341232
## [1423] 0.4755536 0.2918845 0.6070831 0.5379989 0.4879117 0.7085849
## [1429] 0.7066170 0.6727727 0.5470152 0.4281190 0.6765871 0.5948711
## [1435] 0.4947782 0.5108780 0.6217244 0.6558956 0.6395164 0.4041150
## [1441] 0.6172005 0.6309855 0.6781603 0.6498164 0.6498662 0.5342374
## [1447] 0.8334009 0.5482406 0.4184622 0.4782710 0.5924923 0.3666636
## [1453] 0.5185552 0.7025462 0.5469146 0.7506817 0.6386509 0.4737993
## [1459] 0.6216336 0.7657359 0.5148795 0.5912116 0.3577605 0.6621271
## [1465] 0.5108786 0.5399123 0.6057896 0.6251537 0.5814115 0.3979825
## [1471] 0.6930886 0.6495054 0.6615555 0.5114044 0.5818334 0.7553548
## [1477] 0.5261143 0.6506167 0.4294298 0.5194009 0.7064284 0.5524105
## [1483] 0.2940355 0.6885109 0.6414437 0.6592793 0.5003121 0.5467761
## [1489] 0.7075722 0.5605617 0.6613504 0.4588145 0.5686287 0.7056215
## [1495] 0.4697281 0.5613892 0.6393580 0.4785096 0.5548174 0.5042001
## [1501] 0.7560744 0.6352966 0.6133812 0.6083550 0.6079013 0.5886871
## [1507] 0.4458174 0.5234843 0.6875981 0.6584015 0.3753604 0.8053531
## [1513] 0.5879356 0.5083067 0.5862402 0.6574043 0.5010623 0.3604202
## [1519] 0.5515211 0.6717610 0.7437276 0.9135875 0.4929428 0.6300771
## [1525] 0.7538331 0.6343432 0.4597125 0.5714498 0.6678087 0.4972028
## [1531] 0.6586841 0.3789937 0.6773586 0.6347076 0.7585806 0.4908882
## [1537] 0.8176534 0.2934060 0.5816764 0.4534487 0.5789749 0.6070353
## [1543] 0.2988630 0.3149450 0.5910337 0.5592677 0.4654059 0.6596652
## [1549] 0.5921051 0.7885903 0.7758105 0.7743574 0.7543404 0.6336438
## [1555] 0.6098233 0.4326127 0.5484672 0.6018790 0.5271422 0.6221951
## [1561] 0.6985646 0.4906306 0.6725355 0.7929728 0.6216507 0.8236594
## [1567] 0.4177558 0.7870871 0.4900681 0.6425393 0.5605737 0.5830748
## [1573] 0.4777414 0.4725772 0.6290518 0.5444593 0.5451249 0.8574207
## [1579] 0.8301605 0.5490004 0.3579961 0.7888308 0.4147355 0.4290755
## [1585] 0.7655234 0.4533235 0.7483037 0.3780387 0.7257665 0.4915133
## [1591] 0.4801019 0.6314469 0.5730159 0.8598668 0.3194313 0.5025603
## [1597] 0.4152642 0.7426228 0.5631619 0.7430246 0.5364178 0.6293685
## [1603] 0.4700136 0.3630631 0.5932067 0.7343002 0.5773343 0.5652171
## [1609] 0.5548742 0.3636828 0.3855813 0.5857858 0.6575529 0.7800377
## [1615] 0.3101224 0.5317736 0.6206513 0.7516112 0.6632227 0.6589504
## [1621] 0.8852141 0.4989652 0.5147554 0.5505282 0.3831382 0.6691664
## [1627] 0.7318666 0.6621113 0.6731026 0.5163423 0.7348982 0.7512757
## [1633] 0.7576573 0.7675011 0.7268366 0.6127616 0.6759940 0.5653107
## [1639] 0.7420557 0.6329351 0.6650380 0.5512594 0.5945752 0.5142566
## [1645] 0.4226498 0.5811543 0.5869127 0.4980456 0.6065982 0.7296914
## [1651] 0.6028906 0.5621804 0.6024445 0.3373987 0.7342938 0.5853028
## [1657] 0.7208282 0.4149611 0.6549407 0.7332850 0.6452272 0.5885985
## [1663] 0.6917842 0.6194797 0.4739138 0.5936554 0.6080714 0.7723845
## [1669] 0.6148497 0.6256227 0.7467509 0.5897444 0.5891421 0.5815830
## [1675] 0.6932105 0.6675158 0.5447753 0.5244177 0.7574453 0.6696035
## [1681] 0.7887314 0.7690000 0.7620420 0.4798145 0.4974768 0.6145404
## [1687] 0.7436497 0.3221573 0.5821052 0.4805725 0.4964067 0.3752523
## [1693] 0.6741893 0.3621954 0.7672704 0.4317328 0.8429407 0.4732612
## [1699] 0.5083140 0.9008852 0.7097333 0.4959862 0.6189764 0.6293333
## [1705] 0.7280792 0.5168267 0.5868957 0.5588252 0.5940863 0.5824305
## [1711] 0.6213509 0.7270200 0.6589937 0.6018532 0.7267368 0.7308228
## [1717] 0.8159741 0.4510726 0.4976855 0.5286446 0.4193341 0.5800496
## [1723] 0.6082996 0.5626434 0.7782616 0.4814488 0.4406782 0.5497186
## [1729] 0.5859219 0.6895549 0.5180334 0.6658491 0.5634560 0.6253040
## [1735] 0.7619533 0.5465937 0.5854774 0.6041970 0.7828565 0.5763054
## [1741] 0.4593401 0.6835639 0.5786745 0.6928162 0.6356863 0.6935187
## [1747] 0.8135871 0.5196358 0.5617719 0.5913803 0.5458797 0.7520711
## [1753] 0.5926817 0.6513291 0.7039030 0.4901841 0.7140413 0.6047513
## [1759] 0.7321786 0.6310157 0.4006878 0.7583388 0.6900261 0.6723575
## [1765] 0.3481208 0.5567753 0.7078845 0.5448440 0.7341251 0.7561414
## [1771] 0.6557616 0.4849131 0.7249116 0.5228700 0.6809185 0.5133453
## [1777] 0.6030854 0.6255231 0.6394915 0.6630859 0.4527175 0.5295565
## [1783] 0.6215371 0.4997199 0.7193634 0.5643016 0.6431503 0.5929511
## [1789] 0.8689609 0.6164811 0.5560043 0.5609232 0.6557517 0.3715833
## [1795] 0.5523867 0.6272101 0.4461865 0.5515750 0.3286459 0.5934242
## [1801] 0.5419254 0.5440265 0.6861782 0.6840368 0.6162018 0.7047922
## [1807] 0.4641481 0.4454617 0.6668905 0.6900978 0.9026438 0.5106397
## [1813] 0.6039317 0.4569844 0.5472937 0.6819781 0.7883246 0.6750410
## [1819] 0.3238340 0.3910998 0.7401020 0.5594847 0.4932708 0.8978510
## [1825] 0.6077197 0.5804916 0.4165104 0.6242653 0.5958798 0.8326838
## [1831] 0.6447500 0.5621121 0.6347343 0.7153383 0.5808822 0.6115317
## [1837] 0.6633645 0.7351449 0.7146254 0.5115253 0.1153101 0.6166796
## [1843] 0.7849520 0.5687458 0.5532223 0.6500124 0.5978086 0.8347156
## [1849] 0.5911191 0.5694244 0.6248337 0.6305762 0.8122469 0.6795098
## [1855] 0.4136620 0.4003856 0.6664248 0.6630574 0.8300632 0.7064867
## [1861] 0.4824655 0.8724431 0.6963485 0.5512686 0.6982156 0.4953950
## [1867] 0.4581656 0.5660608 0.6874482 0.5899953 0.6063769 0.6276933
## [1873] 0.6451971 0.5613130 0.6349581 0.6512065 0.5261641 0.5147765
## [1879] 0.5284628 0.6747419 0.5847750 0.6631694 0.4789914 0.5552431
## [1885] 0.8749767 0.9152507 0.4405892 0.6538515 0.4830238 0.5897810
## [1891] 0.5885330 0.4537101 0.5918690 0.6257135 0.7361895 0.5706114
## [1897] 0.4717112 0.6973657 0.6234348 0.5109334 0.5121866 0.6792413
## [1903] 0.5726293 0.5500618 0.5020270 0.5551959 0.6080607 0.4004550
## [1909] 0.5874407 0.5794590 0.5580974 0.7279799 0.6149897 0.5597922
## [1915] 0.4657273 0.3740148 0.5965691 0.4032915 0.6287944 0.6780170
## [1921] 0.4755093 0.7612677 0.6245398 0.8505290 0.5009305 0.7154864
## [1927] 0.5550614 0.2565369 0.6045930 0.5187390 0.7196561 0.6794267
## [1933] 0.7079419 0.3200619 0.6045649 0.5622235 0.7362102 0.5456422
## [1939] 0.3753676 0.6718852 0.6317660 0.5852702 0.6125061 0.5344304
## [1945] 0.4033303 0.4856235 0.5925746 0.6459492 0.6345517 0.5510689
## [1951] 0.6668759 0.2649898 0.5226232 0.7539666 0.4551903 0.7373281
## [1957] 0.5312529 0.3457021 0.6918217 0.7929735 0.6134020 0.7165637
## [1963] 0.7333315 0.7462187 0.6276197 0.6391964 0.7483560 0.6375010
## [1969] 0.6531142 0.6231846 0.4985143 0.7456146 0.6518605 0.5513185
## [1975] 0.2738418 0.7385750 0.4713042 0.5597908 0.5358406 0.4831503
## [1981] 0.3437249 0.4417466 0.5762727 0.5872343 0.6371833 0.5822573
## [1987] 0.7732647 0.7392585 0.6816548 0.7322070 0.4607420 0.2947992
## [1993] 0.3983702 0.6234838 0.6892425 0.3468787 0.3177194 0.5000298
## [1999] 0.4110090 0.6226111
```


```r
success <- by(d$y,list(d$Aid,d$Pid,d$Vid),mean)  # calculate the proportion in each combination of dummy variables
head(success)
```

```
## [1]  1 15 20 29  0  1
```

## (b)
Now interpret the estimates. If the quadratic approximation turned out okay, then it’s okay to
use the map estimates. Otherwise stick to map2stan estimates. Then plot the posterior predictions.
Compute and display both (1) the predicted probability of success and its 89% interval for each row (i)
in the data, as well as (2) the predicted success count and its 89% interval What different information
does each type of posterior prediction provide?

Interpretation: Pirating eagle with large body size on average, has 4.68 more successful attempts, with 95% confidence, pirating eagle has 3.27~6.40 more successful attempts. Adult pirating eagle on average has 1.21 more successful attempts, with 95% confidence,adult pirating eagle has 0.39~2.08 more successful attempts. Victim eagle with large body size has 4.97 times less being pirated than victim eagle with small body size.
To sum up, body size of eagle is key determing in success of pirating or protecting food. Age of pirating eagle has a less obvious effect.
not based on the outcome scale

since binomial regression is not guaranteed to produce a multivariate Gaussian posterior distribution, I will stik with the ulam model although their estimates look similar
# ```{r posterior prediction 01}
# success_post <- link_ulam(m11.3.2, data = dat)
# success_mean <- apply(success_post,2,mean)
# success_ci <- apply(success,2,PI)
# ```


```r
postcheck(m11.3.2,n=1e4)
```

![](Chapter11_HW_files/figure-html/unnamed-chunk-13-1.png)<!-- -->

```r
# Blue points are observed proportions admitted for each row in the data, with points from the same department connected by a blue line. Open points, the tiny vertical black lines within them, and the crosses are expected proportions, 89% intervals of the expectation, and 89% interval of simulated samples, respectively.
# the data are roughly within the model prediction
#one datapoint not obviously within it, but not correlated with LooPk value  
```


```r
k <- LOOPk(m11.3.2)
```

```
## Warning: Some Pareto k diagnostic values are too high. See help('pareto-k-diagnostic') for details.
```

```r
plot(dat$n,dat$y, xlab="total number of attempts", ylab="total success", col=rangi2, pch = ifelse(dat$Pid==1,1,16),lwd=2,xlim=c(0,30),ylim=c(0,30),cex=1+normalize(k))
# set up the horizontal axis values to compute predictions at
ns <- 100
n_seq <- seq(from=0,to=30, length.out = ns)
# predictions for pirating with large body size, more mature and victim eagle with small body size
y_101 <- link_ulam(m11.3.2,data = data.frame(n=n_seq, Pid=1, Vid=0, Aid=1)) 
# link_ulam vs link
#, n = data.frame(n=n_seq)) # why 2000 row in y, relationship with n_seq? # cannot use data=data.frame()
y_mu <- apply(y_101,2,mean)
y_ci <- apply(y_101,2,PI,prob=0.89)
#post_2 <- extract.samples(m11.3.2)
lines(n_seq,y_mu,lty=1,lwd=1.5)
shade(y_ci,n_seq,xpd=TRUE)
```

![](Chapter11_HW_files/figure-html/posterior prediction 02-1.png)<!-- -->

##(c) 
Now try to improve the model. Consider an interaction between the pirate’s size and age
(immature or adult). Compare this model to the previous one, using WAIC. Interpret.

```r
dat <- list(y=d$y,n=d$n,Pid=d$Pid,Vid=d$Vid,Aid=d$Aid)
dat$Tid <- d$Pid+d$Aid
m11.3.3 <- ulam(
  alist(
    y ~ dbinom(n,p),
    logit(p) <- a+bT*Tid+bA*Aid, # Tid is aggregated Pid and Aid
    a ~ dnorm(0,1.5),
    bT ~ dnorm(0,1.5),
    bA ~ dnorm(0,1.5)
  ),data=dat, chains = 4,cores = 4, log_lik = TRUE
)
```


```r
plot(precis(m11.3.3,depth = 2))
```

![](Chapter11_HW_files/figure-html/unnamed-chunk-15-1.png)<!-- -->


```r
compare(m11.3.2,m11.3.3, func = WAIC)
```

```
##              WAIC     pWAIC    dWAIC    weight        SE      dSE
## m11.3.2  32.20241  4.671244  0.00000 1.000e+00  5.916859       NA
## m11.3.3 104.91535 20.040973 72.71294 1.624e-16 20.765322 22.10071
```

